{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcc\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "                    ##############################################################################\n",
    "                    ##########                       Imports                          ############\n",
    "                    ##############################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV,StratifiedKFold\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.model_selection import cross_validate, GridSearchCV,StratifiedKFold\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import modularity\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import accuracy_score,log_loss,roc_auc_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from time import time\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "import operator\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from scipy.stats import mode\n",
    "import math\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from ReliefF import ReliefF\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comprehensive guide We have empirically evaluated nine feature selection methods from different categories including: \n",
    "\n",
    "RatlifF, MI and Chi-2 as filters, \n",
    "SFS, SBS and RFE-SVM as wrappers, \n",
    "LASSO, RIDGE and RF selector as embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Cette fonction sert a faire le pretraitement pour tous les datasets '''\n",
    "def load_file(filename):    \n",
    "    #toujours travailler avec des chemins relatifs à la place des chemains absoluts\n",
    "    rep='..\\\\..\\\\'\n",
    "    data=pd.read_csv(rep+filename)\n",
    "    #data.dropna(inplace=True)\n",
    "    if filename == 'wdbc.data' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        #data['diagnostic']=data['diagnostic'].replace('M','N')\n",
    "        #data['diagnostic']=data['diagnostic'].replace('B','P')\n",
    "        data['diagnostic'] = LabelEncoder().fit_transform(data['diagnostic'].astype(str))\n",
    "        Y=data['diagnostic'].values\n",
    "        X=data.drop(['diagnostic'],axis=1).values\n",
    "        #Y = Y[:,np.newaxis]\n",
    "        \n",
    "    if filename == 'wdbc.data' :\n",
    "        \n",
    "        #data['diagnostic'] = LabelEncoder().fit_transform(data['diagnostic'].astype(str))\n",
    "        Y=data.output.values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif filename == 'pima.data' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "        data2=data.values\n",
    "    elif filename == 'ad.data' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        data2=data.values\n",
    "        k=[]\n",
    "        for i in range(1,1559):\n",
    "            k.append('f'+str(i))\n",
    "        k.append(\"class\")\n",
    "        df2 = pd.DataFrame(data=data2,columns=k)\n",
    "        df2['class'] = LabelEncoder().fit_transform(df2['class'].astype(str))\n",
    "        df2=df2.drop(['f1'],axis=1)\n",
    "        df2=df2.drop(['f2'],axis=1)\n",
    "        cpt=[]\n",
    "        for i in df2.columns.values:\n",
    "            for j in range(np.shape(df2)[0]):\n",
    "                if(df2[i][j] == '?'):\n",
    "                    cpt.append(j)\n",
    "        df2=df2.drop(cpt,axis=0)\n",
    "        #df2['class']=df2['class'].replace('ad.',1)\n",
    "        #df2['class']=df2['class'].replace('nonad.',0)\n",
    "        for m in df2.columns.values:\n",
    "            df2[m] = LabelEncoder().fit_transform(df2[m].astype(str))\n",
    "        Y=df2['class'].values\n",
    "        X=df2.drop(['class'],axis=1).values\n",
    "    elif filename=='clean.data':\n",
    "        data.drop(['conformation_name'], axis=1, inplace=True)\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename=='eighthr.data':\n",
    "        #for m in data.columns.values:\n",
    "        #    data[m] = LabelEncoder().fit_transform(data[m].astype(str))\n",
    "        data2=data.values\n",
    "        k=[]\n",
    "        for i in range(1,74):\n",
    "            k.append('f'+str(i))\n",
    "        k.append(\"class\")\n",
    "        df2 = pd.DataFrame(data=data2,columns=k)\n",
    "        df2 = df2.dropna()\n",
    "        for m in df2.columns.values:\n",
    "            df2[m] = LabelEncoder().fit_transform(df2[m].astype(str))\n",
    "        Y=df2['class'].values\n",
    "        X=df2.drop(['class'],axis=1).values\n",
    "    elif filename == 'titanic.csv':\n",
    "        data.drop(['PassengerId'], axis=1, inplace=True)\n",
    "        data['Age'] = data['Age'].fillna(data['Age'].mean())\n",
    "        data.dropna(inplace=True)\n",
    "        Y = data['Survived'].values\n",
    "        X = data.drop('Survived', axis=1).values\n",
    "    elif filename == 'sonar.data' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename=='sick-euthyroid.data':\n",
    "        data.drop('f25',axis=1,inplace=True)\n",
    "        cpt = []\n",
    "        for j in data.columns.values:\n",
    "            for i in range(np.shape(data)[0]):\n",
    "                if(data[j][i] == '?'):\n",
    "                    cpt.append(i)\n",
    "        data.drop(cpt,axis=0,inplace=True)\n",
    "        for m in data.columns.values:\n",
    "            data[m] = LabelEncoder().fit_transform(data[m].astype(str))\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename == 'adult.data' :\n",
    "        for c in data.columns:\n",
    "            data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "        #X = X.values\n",
    "        k =[]\n",
    "        for i in range(np.shape(X)[0]):\n",
    "            for j in range(np.shape(X)[1]):\n",
    "                if(X[i][j]==' ?'):\n",
    "                    k.append(i)\n",
    "        X=np.delete(X,k,0)\n",
    "        Y=np.delete(Y,k,0)\n",
    "    elif filename == 'Connect2.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        data['output'] = LabelEncoder().fit_transform(data['output'])\n",
    "        for m in data.columns.values:\n",
    "            data[m] = LabelEncoder().fit_transform(data[m].astype(str))\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename == 'fothproving.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        data['output'] = LabelEncoder().fit_transform(data['output'])\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename == 'caravan.data' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename == 'quora.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        Y=data['is_duplicate'].values\n",
    "        X=data.drop(['is_duplicate'],axis=1).values\n",
    "    elif filename == 'numerai.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename=='breast-cancer-wisconsin.data':\n",
    "        #data.drop('f16',axis=1,inplace=True)\n",
    "        from scipy.stats import mode\n",
    "        for j in data.columns.values:\n",
    "            x = mode(data[j])[0][0]\n",
    "            #print(x)\n",
    "            data=data.replace({j:'?'},x)\n",
    "        data.ix[:,data.shape[1]-1] = LabelEncoder().fit_transform(data.ix[:,data.shape[1]-1].astype(str))\n",
    "        data['Bare Nuclei'] = LabelEncoder().fit_transform(data['Bare Nuclei'].astype(str))\n",
    "        Y=data.ix[:,data.shape[1]-1].values\n",
    "        X=data.ix[:,:data.shape[1]-1].values\n",
    "    elif filename=='wpbc.data':\n",
    "        data['f34'] = LabelEncoder().fit_transform(data['f34'].astype(str))\n",
    "        data.drop('f1',axis=1,inplace=True)\n",
    "        from scipy.stats import mode\n",
    "        for j in data.columns.values:\n",
    "            x = mode(data[j])[0][0]\n",
    "            #print(x)\n",
    "            data=data.replace({j:'?'},x)\n",
    "        data['label'] = LabelEncoder().fit_transform(data['label'].astype(str))\n",
    "        Y=data['label'].values\n",
    "        X=data.drop(['label'],axis=1).values\n",
    "    elif filename=='bands.data':\n",
    "        #data['f19'] = LabelEncoder().fit_transform(data['f34'].astype(str))\n",
    "        data.drop('feature19',axis=1,inplace=True)\n",
    "        from scipy.stats import mode\n",
    "        for j in data.columns.values:\n",
    "            data[j] = LabelEncoder().fit_transform(data[j].astype(str))\n",
    "        for j in data.columns.values:\n",
    "            x = mode(data[j])[0][0]\n",
    "            data=data.replace({j:'?'},x)\n",
    "        #data['label'] = LabelEncoder().fit_transform(data['label'].astype(str))\n",
    "        Y=data['label'].values\n",
    "        X=data.drop(['label'],axis=1).values\n",
    "    elif filename == 'News_DJIA.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        data=data.dropna()\n",
    "        Y=data['Label'].values\n",
    "        data['Date'] = LabelEncoder().fit_transform(data['Date'])\n",
    "        data['Top1'] = LabelEncoder().fit_transform(data['Top1'].astype(str))\n",
    "        data['Top2'] = LabelEncoder().fit_transform(data['Top2'].astype(str))\n",
    "        data['Top3'] = LabelEncoder().fit_transform(data['Top3'].astype(str))\n",
    "        data['Top4'] = LabelEncoder().fit_transform(data['Top4'].astype(str))\n",
    "        data['Top5'] = LabelEncoder().fit_transform(data['Top5'].astype(str))\n",
    "        data['Top6'] = LabelEncoder().fit_transform(data['Top6'].astype(str))\n",
    "        data['Top7'] = LabelEncoder().fit_transform(data['Top7'].astype(str))\n",
    "        data['Top8'] = LabelEncoder().fit_transform(data['Top8'].astype(str))\n",
    "        data['Top9'] = LabelEncoder().fit_transform(data['Top9'].astype(str))\n",
    "        data['Top10'] = LabelEncoder().fit_transform(data['Top10'].astype(str))\n",
    "        data['Top11'] = LabelEncoder().fit_transform(data['Top11'].astype(str))\n",
    "        data['Top12'] = LabelEncoder().fit_transform(data['Top12'].astype(str))\n",
    "        data['Top13'] = LabelEncoder().fit_transform(data['Top13'].astype(str))\n",
    "        data['Top14'] = LabelEncoder().fit_transform(data['Top14'].astype(str))\n",
    "        data['Top15'] = LabelEncoder().fit_transform(data['Top15'].astype(str))\n",
    "        data['Top16'] = LabelEncoder().fit_transform(data['Top16'].astype(str))\n",
    "        data['Top17'] = LabelEncoder().fit_transform(data['Top17'].astype(str))\n",
    "        data['Top18'] = LabelEncoder().fit_transform(data['Top18'].astype(str))\n",
    "        data['Top19'] = LabelEncoder().fit_transform(data['Top19'].astype(str))\n",
    "        data['To20'] = LabelEncoder().fit_transform(data['Top20'].astype(str))\n",
    "        data['Top21'] = LabelEncoder().fit_transform(data['Top21'].astype(str))\n",
    "        data['Top22'] = LabelEncoder().fit_transform(data['Top22'].astype(str))\n",
    "        data['Top23'] = LabelEncoder().fit_transform(data['Top23'].astype(str))\n",
    "        data['Top24'] = LabelEncoder().fit_transform(data['Top24'].astype(str))\n",
    "        data['Top25'] = LabelEncoder().fit_transform(data['Top25'].astype(str))\n",
    "        X=data.drop(['Label'],axis=1).values\n",
    "    elif filename == 'hepatitis.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        data=data.dropna()\n",
    "        Y=data['output'].values\n",
    "        data['f1'] = LabelEncoder().fit_transform(data['f1'].astype(str))\n",
    "        data['f2'] = LabelEncoder().fit_transform(data['f2'].astype(str))\n",
    "        data['f3'] = LabelEncoder().fit_transform(data['f3'].astype(str))\n",
    "        data['f4'] = LabelEncoder().fit_transform(data['f4'].astype(str))\n",
    "        data['f5'] = LabelEncoder().fit_transform(data['f5'].astype(str))\n",
    "        data['f6'] = LabelEncoder().fit_transform(data['f6'].astype(str))\n",
    "        data['f7'] = LabelEncoder().fit_transform(data['f7'].astype(str))\n",
    "        data['f8'] = LabelEncoder().fit_transform(data['f8'].astype(str))\n",
    "        data['f9'] = LabelEncoder().fit_transform(data['f9'].astype(str))\n",
    "        data['f10'] = LabelEncoder().fit_transform(data['f10'].astype(str))\n",
    "        data['f11'] = LabelEncoder().fit_transform(data['f11'].astype(str))\n",
    "        data['f12'] = LabelEncoder().fit_transform(data['f12'].astype(str))\n",
    "        data['f13'] = LabelEncoder().fit_transform(data['f13'].astype(str))\n",
    "        data['f14'] = LabelEncoder().fit_transform(data['f14'].astype(str))\n",
    "        data['f15'] = LabelEncoder().fit_transform(data['f15'].astype(str))\n",
    "        data['f16'] = LabelEncoder().fit_transform(data['f16'].astype(str))\n",
    "        data['f17'] = LabelEncoder().fit_transform(data['f17'].astype(str))\n",
    "        data['f18'] = LabelEncoder().fit_transform(data['f18'].astype(str))\n",
    "        data['f19'] = LabelEncoder().fit_transform(data['f19'].astype(str))\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename == 'chess.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        data.dropna(inplace=True)\n",
    "        for c in data.columns:\n",
    "            data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename== 'seq.csv' :\n",
    "        data=data.dropna()\n",
    "        data['label'] = LabelEncoder().fit_transform(data['label'])\n",
    "        Y=data['label'].values\n",
    "        X=data.drop(['label'],axis=1).values\n",
    "    elif filename == 'liver.data' :\n",
    "        data['selector']=data['selector'].replace(2,0)\n",
    "        Y=data['selector'].values\n",
    "        X=data.drop(['selector'],axis=1).values\n",
    "    elif filename == 'Hill_Valley_without_noise_Training.data' :\n",
    "        Y=data.ix[:,data.shape[1]-1].values\n",
    "        X=data.ix[:,:data.shape[1]-1].values\n",
    "        #Y = Y[:,np.newaxis]\n",
    "    elif filename== 'letter-recognition.data' :\n",
    "        Y=data.ix[:,0].values\n",
    "        X=data.ix[:,1:].values\n",
    "    elif filename== 'spambase.data' :\n",
    "        data=data.dropna()\n",
    "        data['output'] = LabelEncoder().fit_transform(data['output'])\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename== 'mushrooms.csv' :\n",
    "        data=data.dropna()\n",
    "        data['class'] = LabelEncoder().fit_transform(data['class'])\n",
    "        Y=data['class'].values\n",
    "        X=data.drop(['class'],axis=1).values\n",
    "    elif filename== 'text-similarity.csv' :\n",
    "        data=data.dropna()\n",
    "        data['same_security'] = LabelEncoder().fit_transform(data['same_security'])\n",
    "        Y=data['same_security'].values\n",
    "        X=data.drop(['same_security'],axis=1).values\n",
    "    elif filename== 'weka.csv' :\n",
    "        data=data.dropna()\n",
    "        data['class'] = LabelEncoder().fit_transform(data['class'])\n",
    "        Y=data['class'].values\n",
    "        X=data.drop(['class'],axis=1).values\n",
    "        #X = Imputer().fit_transform(X)\n",
    "    elif filename== 'seattleWeather.csv' :\n",
    "        data=data.dropna()\n",
    "        data['RAIN'] = LabelEncoder().fit_transform(data['RAIN'])\n",
    "        Y=data['RAIN'].values\n",
    "        X=data.drop(['RAIN'],axis=1).values\n",
    "    elif filename== 'horse.csv' :\n",
    "        data['abdomo_appearance'] = LabelEncoder().fit_transform(data['abdomo_appearance'].astype(str))\n",
    "        data['age'] = LabelEncoder().fit_transform(data['age'].astype(str))\n",
    "        data['nasogastric_reflux'] = LabelEncoder().fit_transform(data['nasogastric_reflux'].astype(str))\n",
    "        data['surgery'] = LabelEncoder().fit_transform(data['surgery'].astype(str))\n",
    "        data['temp_of_extremities'] = LabelEncoder().fit_transform(data['temp_of_extremities'].astype(str))\n",
    "        data['peripheral_pulse'] = LabelEncoder().fit_transform(data['peripheral_pulse'].astype(str))\n",
    "        data['capillary_refill_time'] = LabelEncoder().fit_transform(data['capillary_refill_time'].astype(str))\n",
    "        data['pain'] = LabelEncoder().fit_transform(data['pain'].astype(str))\n",
    "        data['peristalsis'] = LabelEncoder().fit_transform(data['peristalsis'].astype(str))\n",
    "        data['abdominal_distention'] = LabelEncoder().fit_transform(data['abdominal_distention'].astype(str))\n",
    "        data['rectal_exam_feces'] = LabelEncoder().fit_transform(data['rectal_exam_feces'].astype(str))\n",
    "        data['abdomen'] = LabelEncoder().fit_transform(data['abdomen'].astype(str))\n",
    "        data['outcome'] = LabelEncoder().fit_transform(data['outcome'].astype(str))\n",
    "        data['surgical_lesion'] = LabelEncoder().fit_transform(data['surgical_lesion'].astype(str))\n",
    "        data['cp_data'] = LabelEncoder().fit_transform(data['cp_data'].astype(str))\n",
    "        data['mucous_membrane'] = LabelEncoder().fit_transform(data['mucous_membrane'].astype(str))\n",
    "        data['nasogastric_tube'] = LabelEncoder().fit_transform(data['nasogastric_tube'].astype(str))\n",
    "        #cpt = []\n",
    "        #for j in data.columns.values:\n",
    "            #for i in range(np.shape(data)[0]):\n",
    "                #if(data[j][i] is np.nan):\n",
    "                    #cpt.append(i)\n",
    "        #data.drop(cpt,axis=0,inplace=True)\n",
    "        from scipy.stats import mode\n",
    "        for j in data.columns.values:\n",
    "            #print (j)\n",
    "            x = mode(data[j])[0][0]\n",
    "            data=data.replace({j:np.nan},x)\n",
    "        #data['outcome'] = LabelEncoder().fit_transform(data['outcome'])\n",
    "        Y=data['outcome'].values\n",
    "        X=data.drop(['outcome'],axis=1).values\n",
    "\n",
    "    elif filename== 'hypothyroid.data' :\n",
    "        #data=data.dropna()\n",
    "        #data['output'] = LabelEncoder().fit_transform(data['output'])\n",
    "        data=data.drop(['f25'],axis=1)\n",
    "        for c in data.columns:\n",
    "            data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "        #X = X.values\n",
    "        k =[]\n",
    "        for i in range(np.shape(X)[0]):\n",
    "            for j in range(np.shape(X)[1]):\n",
    "                if(X[i][j]==' ?'):\n",
    "                    k.append(i)\n",
    "        X=np.delete(X,k,0)\n",
    "        Y=np.delete(Y,k,0)\n",
    "    elif filename== 'sick.data' :\n",
    "        data=data.dropna()\n",
    "        for c in data.columns:\n",
    "            data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n",
    "        data['output'] = LabelEncoder().fit_transform(data['output'])\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename== 'spect.data' :\n",
    "        data=data.dropna()\n",
    "        data['output'] = LabelEncoder().fit_transform(data['output'])\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename== 'house_votes.data' :\n",
    "        data=data.dropna()\n",
    "        data['output'] = LabelEncoder().fit_transform(data['output'])\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename== 'bank-additional-full.csv' :\n",
    "        data['job'] = LabelEncoder().fit_transform(data['job'].astype(str))\n",
    "        data['marital'] = LabelEncoder().fit_transform(data['marital'].astype(str))\n",
    "        data['education'] = LabelEncoder().fit_transform(data['education'].astype(str))\n",
    "        data['default'] = LabelEncoder().fit_transform(data['default'].astype(str))\n",
    "        data['housing'] = LabelEncoder().fit_transform(data['housing'].astype(str))\n",
    "        data['loan'] = LabelEncoder().fit_transform(data['loan'].astype(str))\n",
    "        data['contact'] = LabelEncoder().fit_transform(data['contact'].astype(str))\n",
    "        data['month'] = LabelEncoder().fit_transform(data['month'].astype(str))\n",
    "        data['day_of_week'] = LabelEncoder().fit_transform(data['day_of_week'].astype(str))\n",
    "        data['poutcome'] = LabelEncoder().fit_transform(data['poutcome'].astype(str))\n",
    "        data['duration'] = LabelEncoder().fit_transform(data['duration'].astype(str))\n",
    "        data['previous'] = LabelEncoder().fit_transform(data['previous'].astype(str))\n",
    "        data['nr.employed'] = LabelEncoder().fit_transform(data['nr.employed'].astype(str))\n",
    "        #data=data.replace({'y':'yes'},1)\n",
    "        #data=data.replace({'y':'no'},0)\n",
    "        data=data.dropna()\n",
    "        data['y'] = LabelEncoder().fit_transform(data['y'].astype(str))\n",
    "        Y=data['y'].values\n",
    "        X=data.drop(['y'],axis=1).values\n",
    "        X = Imputer().fit_transform(X)\n",
    "        #Y = Y[:,np.newaxis]\n",
    "    elif filename== 'agaricus-lepiota.data' :\n",
    "        data['class']=data['class'].replace('p','N')\n",
    "        data['class']=data['class'].replace('e','P')\n",
    "        data['class'] = LabelEncoder().fit_transform(data['class'])\n",
    "        data['cap-shape'] = LabelEncoder().fit_transform(data['cap-shape'])\n",
    "        data['cap-surface'] = LabelEncoder().fit_transform(data['cap-surface'])\n",
    "        data['cap-color'] = LabelEncoder().fit_transform(data['cap-color'])\n",
    "        data['bruises'] = LabelEncoder().fit_transform(data['bruises'])\n",
    "        data['odor'] = LabelEncoder().fit_transform(data['odor'])\n",
    "        data['gill-attachment'] = LabelEncoder().fit_transform(data['gill-attachment'])\n",
    "        data['gill-spacing'] = LabelEncoder().fit_transform(data['gill-spacing'])\n",
    "        data['gill-size'] = LabelEncoder().fit_transform(data['gill-size'])\n",
    "        data['gill-color'] = LabelEncoder().fit_transform(data['gill-color'])\n",
    "        data['stalk-shape'] = LabelEncoder().fit_transform(data['stalk-shape'])\n",
    "        data['stalk-root'] = LabelEncoder().fit_transform(data['stalk-root'])\n",
    "        data['stalk-surface-above-ring'] = LabelEncoder().fit_transform(data['stalk-surface-above-ring'])\n",
    "        data['stalk-surface-below-ring'] = LabelEncoder().fit_transform(data['stalk-surface-below-ring'])\n",
    "        data['stalk-color-above-ring'] = LabelEncoder().fit_transform(data['stalk-color-above-ring'])\n",
    "        data['stalk-color-below-ring'] = LabelEncoder().fit_transform(data['stalk-color-below-ring'])\n",
    "        data['veil-type'] = LabelEncoder().fit_transform(data['veil-type'])\n",
    "        data['veil-color'] = LabelEncoder().fit_transform(data['veil-color'])\n",
    "        data['ring-number'] = LabelEncoder().fit_transform(data['ring-number'])\n",
    "        data['ring-type'] = LabelEncoder().fit_transform(data['ring-type'])\n",
    "        data['spore-print-color'] = LabelEncoder().fit_transform(data['spore-print-color'])\n",
    "        data['population'] = LabelEncoder().fit_transform(data['population'])\n",
    "        data['habitat'] = LabelEncoder().fit_transform(data['habitat'])\n",
    "        a = mode(data['stalk-root'])[0][0]\n",
    "        data['stalk-root'].fillna(a, inplace=True)\n",
    "        Y=data['class'].values\n",
    "        X=data.drop(['class'],axis=1).values\n",
    "        #X = Imputer().fit_transform(X)\n",
    "        #Y = Y[:,np.newaxis]\n",
    "    elif filename== 'transfusion.data' :\n",
    "        Y=data.ix[:,data.shape[1]-1].values\n",
    "        X=data.ix[:,:data.shape[1]-1].values\n",
    "        X = Imputer().fit_transform(X)\n",
    "        #Y = Y[:,np.newaxis]\n",
    "    elif filename== 'transfusion2.data' :\n",
    "        Y=data.ix[:,data.shape[1]-1].values\n",
    "        X=data.ix[:,:data.shape[1]-1].values\n",
    "        X = Imputer().fit_transform(X)\n",
    "        #Y = Y[:,np.newaxis]\n",
    "    elif filename== 'playTennis.data' :\n",
    "        Y=data.ix[:,0].values\n",
    "        X=data.ix[:,1:].values\n",
    "        Y = Y[:,np.newaxis]\n",
    "    elif filename== 'monks-1.data' :\n",
    "        Y=data.ix[:,0].values\n",
    "        X=data.ix[:,1:].values\n",
    "    elif filename== 'monks-3.data' :\n",
    "        Y=data.ix[:,0].values\n",
    "        X=data.ix[:,1:].values\n",
    "    elif filename== 'monks-1_2.data' :\n",
    "        Y=data.ix[:,0].values\n",
    "        X=data.ix[:,1:].values\n",
    "    elif filename== 'monks-2.data' :\n",
    "        Y=data.ix[:,0].values\n",
    "        X=data.ix[:,1:].values\n",
    "        #Y = Y[:,np.newaxis]\n",
    "    elif filename == 'Kobe_Bryant_Shot_Selection.csv' :\n",
    "        #utiliser LabeEncoder pour encoder les valeurs catégoriques en valeurs numériques\n",
    "        data=data.dropna()\n",
    "        data['shot_made_flag'] = LabelEncoder().fit_transform(data['shot_made_flag'])\n",
    "        for c in data.columns:\n",
    "            data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n",
    "        Y=data['shot_made_flag'].values\n",
    "        X=data.drop(['shot_made_flag'],axis=1).values\n",
    "    elif filename == 'tictoc.csv':\n",
    "        data['f1'] = LabelEncoder().fit_transform(data['f1'])\n",
    "        data['f2'] = LabelEncoder().fit_transform(data['f2'])\n",
    "        data['f3'] = LabelEncoder().fit_transform(data['f3'])\n",
    "        data['f4'] = LabelEncoder().fit_transform(data['f4'])\n",
    "        data['f5'] = LabelEncoder().fit_transform(data['f5'])\n",
    "        data['f6'] = LabelEncoder().fit_transform(data['f6'])\n",
    "        data['f7'] = LabelEncoder().fit_transform(data['f7'])\n",
    "        data['f8'] = LabelEncoder().fit_transform(data['f8'])\n",
    "        data['f9'] = LabelEncoder().fit_transform(data['f9'])\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "    elif filename == 'sonar.data':\n",
    "        Y=data['output'].values\n",
    "        X=data.drop(['output'],axis=1).values\n",
    "        \n",
    "    elif filename == 'seismic-bumps.data':\n",
    "        data=data.dropna()\n",
    "        #data['shot_made_flag'] = LabelEncoder().fit_transform(data['shot_made_flag'])\n",
    "        for c in data.columns:\n",
    "            data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n",
    "        Y=data.ix[:,data.shape[1]-1].values\n",
    "        X=data.ix[:,:data.shape[1]-1].values\n",
    "    \n",
    "    else :\n",
    "        Y=data.ix[:,data.shape[1]-1].values\n",
    "        X=data.ix[:,:data.shape[1]-1].values\n",
    "        #Y = Y[:,np.newaxis]\n",
    "        \n",
    "    return preprocessing.normalize(X),Y\n",
    "\n",
    "\n",
    "def getData(dataset):\n",
    "    \n",
    "    rep='..\\\\..\\\\'\n",
    "    \n",
    "    for fl in [dataset]:\n",
    "        data = pd.read_csv( rep+fl )\n",
    "        rep=rep\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        if fl=='clean.data':\n",
    "            YY=data.output.reset_index(drop=True)\n",
    "            YY=YY.values\n",
    "            data=data.drop(['conformation_name'],axis=1).reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            skf = StratifiedKFold(n_splits=4)\n",
    "            #skf.get_n_splits(data, YY)\n",
    "            \n",
    "            for train_index, test_index in skf.split(data, YY):\n",
    "            #for a,b in kf:\n",
    "                X_train, XholHout = data.loc[train_index,:].reset_index(drop=True), data.loc[test_index,:].reset_index(drop=True)\n",
    "            \n",
    "                #XholHout = data.loc[b,:].reset_index(drop=True)\n",
    "                #X_train   = data.loc[a,:].reset_index(drop=True)\n",
    "                break\n",
    "            #XholHout = data.loc[b,:].reset_index(drop=True)\n",
    "                \n",
    "        if fl=='chess.csv':\n",
    "            YY=data.output.reset_index(drop=True)\n",
    "            YY=YY.values\n",
    "            for c in data.columns:\n",
    "                data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n",
    "                \n",
    "            skf = StratifiedKFold(n_splits=4)            \n",
    "            for train_index, test_index in skf.split(data, YY):\n",
    "                X_train, XholHout = data.loc[train_index,:].reset_index(drop=True), data.loc[test_index,:].reset_index(drop=True)\n",
    "                \n",
    "        if fl=='Hill_Valley.data':\n",
    "            YY=data.output.reset_index(drop=True)\n",
    "            YY=YY.values\n",
    "            #data=data.drop(['conformation_name'],axis=1).reset_index(drop=True)\n",
    "            kf = StratifiedKFold(YY, n_folds=4, shuffle=True, random_state=0)\n",
    "            for a,b in kf:\n",
    "                XholHout = data[b,:].reset_index(drop=True)\n",
    "                X_train   = data[a,:].reset_index(drop=True)\n",
    "                break\n",
    "        \n",
    "        if fl=='eighth.data':\n",
    "            YY=data.output.reset_index(drop=True)\n",
    "            YY=YY.values\n",
    "            data=data.drop(['id'],axis=1).reset_index(drop=True)\n",
    "            skf = StratifiedKFold(n_splits=4)\n",
    "            #skf.get_n_splits(data, YY)\n",
    "            \n",
    "            for train_index, test_index in skf.split(data, YY):\n",
    "            #for a,b in kf:\n",
    "                X_train, XholHout = data.loc[train_index,:].reset_index(drop=True), data.loc[test_index,:].reset_index(drop=True)\n",
    "            \n",
    "                #XholHout = data.loc[b,:].reset_index(drop=True)\n",
    "                #X_train   = data.loc[a,:].reset_index(drop=True)\n",
    "                break\n",
    "                \n",
    "        if fl=='standard.csv':\n",
    "            \n",
    "            train = pd.read_csv(rep+fl)\n",
    "\n",
    "            remove = []\n",
    "            for col in train.columns:\n",
    "                if train[col].std() == 0:\n",
    "                    remove.append(col)\n",
    "\n",
    "            train.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "            # remove duplicated columns\n",
    "            remove = []\n",
    "            cols = train.columns\n",
    "            for i in range(len(cols)-1):\n",
    "                v = train[cols[i]].values\n",
    "                for j in range(i+1,len(cols)):\n",
    "                    if np.array_equal(v,train[cols[j]].values):\n",
    "                        remove.append(cols[j])\n",
    "\n",
    "            train.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "            data = train.drop([\"TARGET\",\"ID\"],axis=1)\n",
    "            YY = train.TARGET.values\n",
    "\n",
    "            skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "            for train_index, test_index in skf.split(data, YY):\n",
    "                X_train, XholHout = data.loc[train_index,:].reset_index(drop=True), data.loc[test_index,:].reset_index(drop=True)\n",
    "\n",
    "                break\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        else:\n",
    "            \n",
    "            YY=data.output.reset_index(drop=True)\n",
    "            YY=YY.values\n",
    "            \n",
    "            \n",
    "            skf = StratifiedKFold(n_splits=3)\n",
    "            #skf.get_n_splits(data, YY)\n",
    "            \n",
    "            for train_index, test_index in skf.split(data, YY):\n",
    "            #for a,b in kf:\n",
    "                X_train, XholHout = data.loc[train_index,:].reset_index(drop=True), data.loc[test_index,:].reset_index(drop=True)\n",
    "            \n",
    "                #XholHout = data.loc[b,:].reset_index(drop=True)\n",
    "                #X_train   = data.loc[a,:].reset_index(drop=True)\n",
    "                break\n",
    "            \n",
    "    \n",
    "                \n",
    "                \n",
    "        X=X_train.drop(['output'],axis=1)\n",
    "        X    = X.values\n",
    "        Y=X_train.output.values\n",
    "        Y_holHout=XholHout.output.values\n",
    "        X_holHout=XholHout.drop(['output'],axis=1)\n",
    "        X_holHout=X_holHout.values\n",
    "        feature_scaler = StandardScaler()  \n",
    "        #XX = feature_scaler.fit_transform(X)\n",
    "        #XX_hold=feature_scaler.fit_transform(X_holHout),\n",
    "         #np.asarray(x, dtype='float64')\n",
    "    return preprocessing.normalize(X),Y,preprocessing.normalize(X_holHout),Y_holHout\n",
    "\n",
    "def getMadelon():\n",
    "    \n",
    "    X_train = pd.read_csv('/madelon/madelon_train.data',delimiter=' ', header=None)\n",
    "    Y_train=pd.read_csv('/madelon/madelon_train.labels',delimiter=' ', header=None, names=['target'])\n",
    "    \n",
    "    Y_train.rename(columns={0:'target'}, inplace=True)\n",
    "    madelon_train_total = pd.concat([X_train, Y_train], axis=1)\n",
    "    madelon_train_total.drop([500,'target'], axis=1, inplace=True)\n",
    "    \n",
    "    #madelon_sample_train = madelon_total_samples.drop([500,'target'], axis=1)\n",
    "#madelon_sample_train_labels = madelon_total_samples['target']\n",
    "    \n",
    "    X_test = pd.read_csv('/madelon/madelon_valid.data',delimiter=' ', header=None)\n",
    "    Y_test=pd.read_csv('/madelon/madelon_valid.labels',delimiter=' ', header=None, names=['target'])\n",
    "    \n",
    "    Y_test.rename(columns={0:'target'}, inplace=True)\n",
    "    madelon_valid_total = pd.concat([X_test, Y_test], axis=1)\n",
    "    madelon_valid_total.drop([500,'target'], axis=1, inplace=True)\n",
    "    \n",
    "        #X=data.values\n",
    "    return madelon_train_total,madelon_valid_total,Y_train.values.ravel(),Y_test.values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, nine binary classification benchmark datasets have been used. The datasets are chosen to be different in terms of the number of instances and attributes to validate the effectiveness and the efficiency of the previously discussed algorithms. All datasets are well-known in the field of\n",
    "feature selection, and they can be downloaded from UCI machine learning repository or the Kaggle platform. Table 2 shows more precise information about the characteristics of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAT SET SHAPE :\n",
      "----------\n",
      "    Train : (103, 19) (103,)\n",
      "    TEST : (26, 19) (26,)\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "############## The used datasets ############\n",
    "\n",
    "'chess.csv'\n",
    "'spambase.data'\n",
    "'clean.data'\n",
    "'madelon.csv'\n",
    "'sonar.data'\n",
    "'credit card clients.csv'\n",
    "'caravan.data'\n",
    "'ionosphere.csv'\n",
    "'ds1.100.csv'\n",
    "'eighthr.data'\n",
    "'hepatitis.csv'\n",
    "'sonar.data'\n",
    "#############\n",
    "\n",
    "dataset='hepatitis.csv'\n",
    "\n",
    "print ('DATAT SET SHAPE :')\n",
    "print('----------')\n",
    "#X_train,  y_train,X_test, y_test=getData(dataset)\n",
    "#x,Y=load_file(dataset)\n",
    "#X_train, X_test, y_train, y_test =getMadelon()\n",
    "\n",
    "#X=preprocessing.normalize(x)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "print ('    Train :',X_train.shape,y_train.shape)\n",
    "print ('    TEST :',X_test.shape,y_test.shape)\n",
    "print ('-----------')\n",
    "#Matrix_weight=load_traind_models('madelon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Baseline Model- Without Feature Selection #################################################\n",
    "#######################################################################################################################\n",
    "clf = [svm.SVC(kernel='linear', C=1),RandomForestClassifier(max_depth=3,n_estimators=100,random_state=42),KNeighborsClassifier(n_neighbors=5)]\n",
    "#DecisionTreeClassifier(max_depth=5,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Accuracy: 0.63 (+/- 0.07)\n",
      "---Accuracy: 0.71 (+/- 0.07)\n",
      "---Accuracy: 0.82 (+/- 0.07)\n",
      "\n",
      "** Time execution of (RF) is:  141.20283341407776\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:\n",
      "\n",
      "---Accuracy of: 0.97 (+/- 0.00)\n",
      "---Accuracy of: 0.97 (+/- 0.00)\n",
      "---Accuracy of: 0.96 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "print('Baseline:')\n",
    "print()\n",
    "for i in clf:\n",
    "    #print (i.fit(X_train, y_train).score(X_test, y_test))\n",
    "    scores = cross_val_score(i, X_test, y_test, cv=5)\n",
    "    \n",
    "    print(\"---Accuracy of: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################  Model- With Feature Selection #################################################\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################  Filter Methods (ReliefE, chi2 and Mutual Information) #######################################\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### RELIfF #################################################\n",
    "############################################################################################################\n",
    "start=time.time()\n",
    "print('RELIEfE:')\n",
    "print()\n",
    "from ReliefF import ReliefF\n",
    "fs = ReliefF(n_neighbors=int(0.1*X_train.shape[0]), n_features_to_keep=15)\n",
    "X_ReliefF = fs.fit_transform(X_test, y_test)\n",
    "for i in clf:\n",
    "    \n",
    "    scoresReliefF = cross_val_score(i, X_ReliefF, y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresReliefF.mean(), scoresReliefF.std() * 2))\n",
    "\n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of RelifF is: ', end-start)\n",
    "############################################################################################################\n",
    "\n",
    "################################################### Mutual Information #####################################\n",
    "############################################################################################################\n",
    "start=time.time()\n",
    "print()\n",
    "print('Mutual Information\"(\" MI\")\":')\n",
    "print()\n",
    "testMI = SelectKBest(score_func=mutual_info_classif, k=15)\n",
    "fit = testMI.fit(X_train, y_train)\n",
    "X_MI=testMI.fit_transform(X_test,y_test)\n",
    "for i in clf:\n",
    "\n",
    "    scoresKbest = cross_val_score(i, X_MI, y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresKbest.mean(), scoresKbest.std() * 2))\n",
    "\n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of MI is: ', end-start)\n",
    "    \n",
    "################################################### Chi-Square #############################################\n",
    "############################################################################################################\n",
    "start=time.time()\n",
    "print() \n",
    "print('Chi-square:')\n",
    "print()\n",
    "testKbest = SelectKBest(score_func=chi2, k=15)\n",
    "fit = testKbest.fit(X_train, y_train)\n",
    "X_kbest=testKbest.fit_transform(X_test, y_test)\n",
    "for i in clf:\n",
    "\n",
    "    scoresKbest = cross_val_score(i, X_kbest, y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresKbest.mean(), scoresKbest.std() * 2))\n",
    "    \n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of Chi-squire is: ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################  Wrapper Methods (SFS, SBS, RFE) ###############################################\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFS\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    1.8s finished\n",
      "\n",
      "[2021-02-25 22:31:27] Features: 1/15 -- score: 0.622[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 499 out of 499 | elapsed:    2.6s finished\n",
      "\n",
      "[2021-02-25 22:31:30] Features: 2/15 -- score: 0.653[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 498 out of 498 | elapsed:    3.5s finished\n",
      "\n",
      "[2021-02-25 22:31:33] Features: 3/15 -- score: 0.6765000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 497 out of 497 | elapsed:    2.8s finished\n",
      "\n",
      "[2021-02-25 22:31:36] Features: 4/15 -- score: 0.6835[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 496 out of 496 | elapsed:    2.7s finished\n",
      "\n",
      "[2021-02-25 22:31:39] Features: 5/15 -- score: 0.7060000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 495 out of 495 | elapsed:    3.3s finished\n",
      "\n",
      "[2021-02-25 22:31:42] Features: 6/15 -- score: 0.7125[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 494 out of 494 | elapsed:    3.8s finished\n",
      "\n",
      "[2021-02-25 22:31:46] Features: 7/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 493 out of 493 | elapsed:    4.6s finished\n",
      "\n",
      "[2021-02-25 22:31:51] Features: 8/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 492 out of 492 | elapsed:    5.7s finished\n",
      "\n",
      "[2021-02-25 22:31:57] Features: 9/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 491 out of 491 | elapsed:    4.7s finished\n",
      "\n",
      "[2021-02-25 22:32:02] Features: 10/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 490 out of 490 | elapsed:    5.0s finished\n",
      "\n",
      "[2021-02-25 22:32:07] Features: 11/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 489 out of 489 | elapsed:    4.9s finished\n",
      "\n",
      "[2021-02-25 22:32:12] Features: 12/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 488 out of 488 | elapsed:    5.0s finished\n",
      "\n",
      "[2021-02-25 22:32:17] Features: 13/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 487 out of 487 | elapsed:    5.1s finished\n",
      "\n",
      "[2021-02-25 22:32:22] Features: 14/15 -- score: 0.7140000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:    5.2s finished\n",
      "\n",
      "[2021-02-25 22:32:28] Features: 15/15 -- score: 0.7140000000000001"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Accuracy: 0.58 (+/- 0.06)\n",
      "---Accuracy: 0.63 (+/- 0.05)\n",
      "---Accuracy: 0.75 (+/- 0.08)\n",
      "\n",
      "** Time execution of (SFS) is:  108.59883403778076\n",
      "SBS\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "\n",
      "[2021-02-25 22:35:27] Features: 499/15 -- score: 0.7095[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 499 out of 499 | elapsed:  2.2min finished\n",
      "\n",
      "[2021-02-25 22:37:39] Features: 498/15 -- score: 0.7105[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 498 out of 498 | elapsed:  2.2min finished\n",
      "\n",
      "[2021-02-25 22:39:50] Features: 497/15 -- score: 0.7110000000000001[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 497 out of 497 | elapsed:  2.2min finished\n",
      "\n",
      "[2021-02-25 22:42:02] Features: 496/15 -- score: 0.7114999999999999[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 496 out of 496 | elapsed:  2.2min finished\n",
      "\n",
      "[2021-02-25 22:44:12] Features: 495/15 -- score: 0.7114999999999999[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 495 out of 495 | elapsed:  2.1min finished\n",
      "\n",
      "[2021-02-25 22:46:21] Features: 494/15 -- score: 0.7114999999999999[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 494 out of 494 | elapsed:  2.3min finished\n",
      "\n",
      "[2021-02-25 22:48:40] Features: 493/15 -- score: 0.7114999999999999[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   10.0s\n",
      "\n",
      "STOPPING EARLY DUE TO KEYBOARD INTERRUPT..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Accuracy: 0.55 (+/- 0.06)\n",
      "---Accuracy: 0.62 (+/- 0.05)\n",
      "---Accuracy: 0.63 (+/- 0.09)\n",
      "\n",
      "** Time execution of (SBS) is:  968.2818865776062\n"
     ]
    }
   ],
   "source": [
    "###################################### SFS #################################################################\n",
    "############################################################################################################\n",
    "start=time.time()\n",
    "print('SFS')\n",
    "print()\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=2) # ml_algo used = knn\n",
    "rf=DecisionTreeClassifier(max_depth=3,random_state=42)#RandomForestClassifier(max_depth=4,n_estimators=20)\n",
    "sfs = SFS(rf, \n",
    "           k_features=15, \n",
    "           forward=True, # if forward = True then SFS otherwise SBS\n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "          n_jobs=-1\n",
    "           )\n",
    "#after applying sfs fit the data:\n",
    "sfs.fit(X_train, y_train)\n",
    "sfs_features=[]\n",
    "for i in sfs.k_feature_names_:\n",
    "    sfs_features.append(int(i))\n",
    "\n",
    "for i in clf:\n",
    "    scoresSFS = cross_val_score(i, X_test[sfs_features], y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresSFS.mean(), scoresSFS.std() * 2))\n",
    "\n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of (SFS) is: ', end-start)\n",
    "\n",
    "###################################### SBS #################################################################\n",
    "############################################################################################################\n",
    "start=time.time()\n",
    "print('SBS')\n",
    "print()\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=2) # ml_algo used = knn\n",
    "rf=DecisionTreeClassifier(max_depth=3,random_state=42)#RandomForestClassifier(max_depth=4,n_estimators=2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=2) # ml_algo used = knn\n",
    "sbs = SFS(rf, \n",
    "           k_features=15, \n",
    "           forward=False, # if forward = True then SFS otherwise SBS\n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "          n_jobs=-1\n",
    "           )\n",
    "#after applying sfs fit the data:\n",
    "sbs.fit(X_train, y_train)\n",
    "sbs_features=[]\n",
    "for i in sbs.k_feature_names_:\n",
    "    sbs_features.append(int(i))\n",
    "\n",
    "for i in clf:\n",
    "\n",
    "    scoresSBS = cross_val_score(i, X_test[sbs_features], y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresSBS.mean(), scoresSBS.std() * 2))\n",
    "\n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of (SBS) is: ', end-start)\n",
    "\n",
    "\n",
    "###################################### RFE-SVM ######################################################################\n",
    "#####################################################################################################################\n",
    "print('RFE-SVM:')\n",
    "print()\n",
    "\n",
    "start=time.time()\n",
    "#from sklearn.feature_selection import \n",
    "\n",
    "#from sklearn.feature_selection import RFE\n",
    "estimator = svm.SVC(kernel='linear', C=1)#RandomForestClassifier(random_state = 42)\n",
    "#selector = SVR(kernel=\"linear\")\n",
    "selector=RFE(estimator, n_features_to_select=15, step=1)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "rfe_mask = selector.get_support() #list of booleans for selected features\n",
    "new_features = [] \n",
    "for bool, feature in zip(rfe_mask, [i for i in range(X_train.shape[1])]):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "\n",
    "for i in clf:\n",
    "    \n",
    "    scoresRFE = cross_val_score(i, X_test[:,new_features], y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresRFE.mean(), scoresRFE.std() * 2))\n",
    "\n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of (RFE-SVM) is: ', end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################  Embedded Methods (LASSO, RIDGE, RFE-SVM) ###############################################\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO:\n",
      "\n",
      "---Accuracy: 0.61 (+/- 0.04)\n",
      "---Accuracy: 0.64 (+/- 0.05)\n",
      "---Accuracy: 0.75 (+/- 0.04)\n",
      "\n",
      "** Time execution of (LASSO) is:  572.879486322403\n",
      "RIDGE:\n",
      "\n",
      "---Accuracy: 0.53 (+/- 0.06)\n",
      "---Accuracy: 0.53 (+/- 0.08)\n",
      "---Accuracy: 0.54 (+/- 0.11)\n",
      "\n",
      "** Time execution of (RIDGE) is:  576.7905640602112\n"
     ]
    }
   ],
   "source": [
    "#################################################### Random Forest (RF) ##################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "print('RF:')\n",
    "print()\n",
    "# random forest for feature importance on a classification problem\n",
    "start=time.time()\n",
    "print()\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "f=dict()\n",
    "for i in range(len(importance)):\n",
    "    f[i]=importance[i]\n",
    "A=sorted(f.items(),key=operator.itemgetter(1),reverse=True)[:15]\n",
    "RFsubset=[i[0] for i in A]\n",
    "for i in clf:\n",
    "    \n",
    "    RFscores = cross_val_score(i, X_test[RFsubset], y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (RFscores.mean(), RFscores.std() * 2))\n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of (RF) is: ', end-start)\n",
    "print()\n",
    "#################################################### Lassoo #############################################################\n",
    "#########################################################################################################################\n",
    "print('LASSO:')\n",
    "print()\n",
    "start=time.time()\n",
    "def lasso_selection(X,y):\n",
    "    \n",
    "    from sklearn.linear_model import Ridge,Lasso\n",
    "    \n",
    "    param_grid ={'alpha':[0.1,0.01,1.,1,2],'max_iter':[10000]}\n",
    "    rr = Lasso()\n",
    "    grid_search = GridSearchCV(estimator = rr, param_grid = param_grid, \n",
    "                              cv = 3, n_jobs = -4)\n",
    "    grid_search.fit(X, y)\n",
    "    best_params=grid_search.best_params_\n",
    "    rr = Lasso(alpha=best_params['alpha'],max_iter=best_params['max_iter']) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "    rr.fit(X, y)\n",
    "    \n",
    "    feat=[i for i in range(X.shape[1])]\n",
    "    dictionary=dict(zip(feat,rr.coef_))\n",
    "    #print(dictionary)\n",
    "    \n",
    "    dset = pd.DataFrame()\n",
    "    dset['attr'] = dictionary.keys()\n",
    "    dset['importance'] =dictionary.values()\n",
    "    dset.sort_values(by='importance', ascending=False,inplace=True)\n",
    "    \n",
    "    return [row for row in dset.index]\n",
    "LassoFeatures=lasso_selection(X_train,y_train)[:15]\n",
    "for i in clf:\n",
    "    \n",
    "    scoresLasso = cross_val_score(i, X_train[LassoFeatures], y_train, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresLasso.mean(), scoresLasso.std() * 2))\n",
    "    \n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of (LASSO) is: ', end-start)\n",
    "################################################################ RIDGE ##################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "print('RIDGE:')\n",
    "print()\n",
    "def ridge_selection(X,y):\n",
    "    \n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    param_grid ={'alpha':[0.1,0.01,0.001,1.,10.]}\n",
    "    rr = Ridge()\n",
    "    grid_search = GridSearchCV(estimator = rr, param_grid = param_grid, \n",
    "                              cv = 3, n_jobs = -4)\n",
    "    grid_search.fit(X, y)\n",
    "    best_params=grid_search.best_params_\n",
    "    rr = Ridge(alpha=best_params['alpha']) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "    rr.fit(X, y)\n",
    "    \n",
    "    feat=[i for i in range(X.shape[1])]\n",
    "    dictionary=dict(zip(feat,rr.coef_))\n",
    "    #print(dictionary)\n",
    "    \n",
    "    dset = pd.DataFrame()\n",
    "    dset['attr'] = dictionary.keys()\n",
    "    dset['importance'] =dictionary.values()\n",
    "    dset.sort_values(by='importance', ascending=False,inplace=True)\n",
    "    \n",
    "    return [row for row in dset.index]\n",
    "\n",
    "RidgeFeatures=ridge_selection(X_train,y_train)[:15]\n",
    "for i in clf:\n",
    "    \n",
    "    scoresridge = cross_val_score(i, X_test[RidgeFeatures], y_test, cv=5)\n",
    "    print(\"---Accuracy: %0.2f (+/- %0.2f)\" % (scoresridge.mean(), scoresridge.std() * 2))\n",
    "\n",
    "end=time.time()  \n",
    "print()\n",
    "print('** Time execution of (RIDGE) is: ', end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
